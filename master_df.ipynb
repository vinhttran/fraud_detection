{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load helper_functions2.py\n",
    "\n",
    "def user_event_count():\n",
    "    df['user_event_count'] = df.groupby('user_created').count().max(axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def previous_payouts(df):\n",
    "    new_df=pd.DataFrame(df.previous_payouts[0])\n",
    "    for i in range(1,df.previous_payouts.shape[0]):\n",
    "        df_element=pd.DataFrame(df.previous_payouts[i])\n",
    "        new_df=pd.concat([new_df, df_element])\n",
    "    return new_df\n",
    "\n",
    "def user_df():\n",
    "    unique_cols = []\n",
    "    for col in columns:\n",
    "        if unique_col_indicator(df,col):\n",
    "            unique_cols.append(col)\n",
    "    user_unique_events = df.groupby('user_created').count().max(axis=1)\n",
    "    user_unique_events = pd.DataFrame(user_unique_events,columns=['user_event_count'])\n",
    "    return df[unique_cols].merge(user_unique_events,how='inner',on='user_created')\n",
    "\n",
    "def unique_col_indicator(df,col):\n",
    "    try:\n",
    "        col_count = df.groupby(['user_created',col])['channels'].count().shape[0]\n",
    "        user_count = df.groupby(['user_created'])['channels'].count().shape[0]\n",
    "        if col_count == user_count:\n",
    "            return True\n",
    "        else:\n",
    "\n",
    "            return False\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    \n",
    "#check\n",
    "def user_unique_events_col():\n",
    "    user_unique_events = df.groupby('user_created').count().max(axis=1)\n",
    "    user_unique_events =  pd.DataFrame(user_unique_events,columns=['user_event_count'])\n",
    "    return df.merge(user_unique_events,how='inner',on='user_created')\n",
    "\n",
    "#check\n",
    "def create_target():\n",
    "    df['fraud'] = df['acct_type'].map({'fraudster_event': 1,\n",
    "                                   'premium': 0,\n",
    "                                   'spammer_warn': 0,\n",
    "                                   'fraudster': 1,\n",
    "                                   'spammer_limited': 0,\n",
    "                                   'spammer_noinvite': 0,\n",
    "                                   'locked': 1,\n",
    "                                   'tos_lock': 0,\n",
    "                                   'tos_warn': 0,\n",
    "                                   'fraudster_att': 1,\n",
    "                                   'spammer_web': 0,\n",
    "                                   'spammer': 0})\n",
    "    return df\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "#check\n",
    "def convert_date():\n",
    "    df[\"approx_payout_date\"] = pd.to_datetime(df[\"approx_payout_date\"], unit = 's')\n",
    "    df[\"event_created\"] = pd.to_datetime(df[\"event_created\"], unit = 's')\n",
    "    df[\"event_end\"] = pd.to_datetime(df[\"event_end\"], unit = 's')\n",
    "    df[\"event_start\"] = pd.to_datetime(df[\"event_start\"], unit = 's')\n",
    "    df[\"event_published\"] = pd.to_datetime(df[\"event_published\"], unit = 's')\n",
    "    return df\n",
    "\n",
    "def low_cor_cols(r_score):\n",
    "    df[num_cols()].corr()[['fraud']].values\n",
    "    corr_df = df[num_cols()].corr()[['fraud']].sort_values('fraud')\n",
    "    low_corr_mask = corr_df.sort_values('fraud').abs().lt(r_score).values.reshape(1,-1)[0]\n",
    "    return corr_df[low_corr_mask]\n",
    "\n",
    "def get_corrs():\n",
    "    corr_df = df[num_cols()].corr()[['fraud']].abs().sort_values('fraud')\n",
    "    return corr_df\n",
    "\n",
    "def num_cols():\n",
    "    return df.describe().columns\n",
    "\n",
    "def drop_cols():\n",
    "    df1 = df.drop(low_cor_cols(.09).index.tolist(),axis=1)\n",
    "    df2 = df1.drop(['acct_type','sale_duration','fraud','event_start','approx_payout_date',\n",
    "                    'event_published','event_end','venue_state',\n",
    "                    'ticket_types','venue_name','venue_state','venue_country','venue_address',\n",
    "                    'org_desc','org_name','previous_payouts','email_domain','payee_name','name','currency','country','event_created'],axis=1)\n",
    "    return df2\n",
    "\n",
    "#check\n",
    "def description_cols():\n",
    "    import clean_desc\n",
    "    df['description'] = df['description'].apply(strip_tags)\n",
    "    clean_desc(df)\n",
    "    return df\n",
    "\n",
    "#check\n",
    "def ticket_types(df):\n",
    "    new_df=pd.DataFrame(df.ticket_types[0])\n",
    "    result_df=new_df[['quantity_sold', 'quantity_total', 'event_id']].groupby('event_id').sum()\n",
    "    result_df=result_df.join(new_df[['availability', 'cost', 'event_id']].groupby('event_id').mean())\n",
    "\n",
    "    for i in range(1,df.shape[0]):\n",
    "        try:\n",
    "            df_element=pd.DataFrame(df.ticket_types[i])\n",
    "            grp_ele_df=df_element[['quantity_sold', 'quantity_total', 'event_id']].groupby('event_id').sum()\n",
    "            grp_ele_df=grp_ele_df.join(df_element[['availability', 'cost', 'event_id']].groupby('event_id').mean())\n",
    "            result_df=pd.concat([result_df, grp_ele_df])\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "    result_df.reset_index(inplace=True)\n",
    "    result_df.columns=['object_id', 'quantity_sold', 'quantity_total', 'availability', 'cost']\n",
    "    final_df=df.set_index('object_id').join(result_df.set_index('object_id'))\n",
    "    final_df.reset_index(inplace=True)\n",
    "    final_df[['quantity_sold', 'quantity_total','availability', 'cost']]=final_df[['quantity_sold', 'quantity_total','availability', 'cost']].fillna(0)\n",
    "    return final_df\n",
    "\n",
    "def timedelta(field):\n",
    "    return field.days\n",
    "\n",
    "def event_times():\n",
    "    df['publish_time_to_start'] =  df['event_published'] - df['event_created'] \n",
    "    df['publish_time_to_start'] = df['publish_time_to_start'].apply(timedelta)\n",
    "    return df\n",
    "\n",
    "def has_venue_data():\n",
    "    df['has_state'] = (df['venue_state'].fillna('None').replace('','None') == 'None').astype(int)\n",
    "    df['has_country'] = (df['venue_country'].fillna('None').replace('','None') == 'None').astype(int)\n",
    "    df['has_address'] = (df['venue_address'].fillna('None').replace('','None') == 'None').astype(int)\n",
    "    df['has_venue_name'] = (df['venue_name'].fillna('None').replace('','None') == 'None').astype(int)\n",
    "    \n",
    "    df['has_org_desc'] = (df['org_desc'].fillna('None').replace('','None') == 'None').astype(int)\n",
    "    df['has_org_name'] = (df['org_name'].fillna('None').replace('','None') == 'None').astype(int)\n",
    "    df['has_venue_name'] = (df['venue_name'].fillna('None').replace('','None') == 'None').astype(int)\n",
    "    df['has_venue_name'] = (df['venue_name'].fillna('None').replace('','None') == 'None').astype(int)\n",
    "    df['has_payee_name'] = (df['payee_name'].fillna('None').replace('','None') == 'None').astype(int)\n",
    "    df['org_facebook'] = df['org_facebook'].fillna(df['org_facebook'].mode()[0])\n",
    "    df['org_twitter'] = df['org_twitter'].fillna(df['org_twitter'].mode()[0])\n",
    "    return df\n",
    "\n",
    "def payout_type():\n",
    "    df1 = pd.get_dummies(df,columns = ['payout_type','listed'])\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['org_twitter'].mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', 500)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "pd.set_option('max_colwidth', 40)\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "sws = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "from  helper_functions import user_event_count,\\\n",
    "                                ticket_types,\\\n",
    "                                previous_payouts,\\\n",
    "                                user_df,\\\n",
    "                                unique_col_indicator,\\\n",
    "                                create_target,\\\n",
    "                                convert_date,\\\n",
    "                                low_cor_cols,\\\n",
    "                                drop_cols,\\\n",
    "                                description_cols,\\\n",
    "                                ticket_types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('data/data.json').drop('description',axis=1)\n",
    "df = ticket_types(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = convert_date()\n",
    "df = create_target()\n",
    "df = user_unique_events_col()\n",
    "df = event_times()\n",
    "df = has_venue_data()\n",
    "df = payout_type()\n",
    "df = drop_cols()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('data/df_final.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('delivery_method',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_json('data/data.json').drop(['description'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['fraud'] = df2['acct_type'].map({'fraudster_event': 1,\n",
    "                                   'premium': 0,\n",
    "                                   'spammer_warn': 0,\n",
    "                                   'fraudster': 1,\n",
    "                                   'spammer_limited': 0,\n",
    "                                   'spammer_noinvite': 0,\n",
    "                                   'locked': 1,\n",
    "                                   'tos_lock': 0,\n",
    "                                   'tos_warn': 0,\n",
    "                                   'fraudster_att': 1,\n",
    "                                   'spammer_web': 0,\n",
    "                                   'spammer': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y = df2['fraud']\n",
    "X = df\n",
    "ss= StandardScaler()\n",
    "X = ss.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=0, solver='sag',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=0,solver='sag')\n",
    "lr.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95      3585\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      3585\n",
      "   macro avg       0.50      0.45      0.48      3585\n",
      "weighted avg       1.00      0.91      0.95      3585\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = lr.predict(x_test)\n",
    "print(classification_report(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(x_train,y_train)\n",
    "y_pred = rf.predict(x_test)\n",
    "y_pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95      3575\n",
      "           1       0.01      0.40      0.02        10\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      3585\n",
      "   macro avg       0.51      0.65      0.49      3585\n",
      "weighted avg       1.00      0.91      0.95      3585\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_vector = pd.read_pickle('data/description_topics_df.pickle')\n",
    "X = topic_vector\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96      3533\n",
      "           1       0.15      0.96      0.25        52\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      3585\n",
      "   macro avg       0.57      0.94      0.61      3585\n",
      "weighted avg       0.99      0.92      0.95      3585\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf.predict(x_test)\n",
    "print(classification_report(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9615384615384616"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[y_pred==1].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
